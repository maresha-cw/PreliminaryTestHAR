import numpy as np
import pandas as pd
import random
import cv2
import os
import functools
from imutils import paths
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.metrics import accuracy_score

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D
from tensorflow.keras.layers import MaxPooling1D
from tensorflow.keras.layers import Activation
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import SGD
from tensorflow.keras import backend as K

from google.colab import drive
drive.mount('/content/drive')
directori = "/content/drive/My Drive/testGC/DBALab-Test/Preprocessed/"

## Read data per subject
def read_subject(subject):
  file_path = directori + 'mHealth_subject' + str(subject) + '.csv'
  # Read file
  try:
    df = pd.read_csv(file_path, delim_whitespace = True, header = None)
  except IOError:
    print("Data file does not exist!")
  # Remove data with null class (=0)
  df = df[df[23] > 0]
  df = df.head(5000).copy()
  return df

def split_by_blocks(df, block_size=100):
  # Channels, block_size
  n_channels = df.shape[1]-1
  block_size = 100
  # Group by labels
  grps = df.groupby(21)
  # Create a list for concatenating
  X_ = []
  Y_ = []
  # Loop over groups (labels), reshape to tensor and concatenate
  for ig in range(1,7,1):
    df_ = df
    # Data and targets
    y = pd.unique(df_[21].values)
    x = df_.drop(21, axis=1).values
    
    n_blocks = len(x) // block_size
    x = x[:n_blocks*block_size]
    y = y[:n_blocks*block_size]

    x_tensor = x.reshape(-1, block_size, n_channels)

    # Append
    X_.append(x_tensor)
    Y_.append(np.array([y]*len(x_tensor), dtype=int).squeeze())

  # Concatenate and return
  X = np.concatenate(X_, axis=0)
  Y = np.concatenate(Y_, axis=0)

  return X, Y

def collect_save_data(subject_count = 10, block_size=100):
  """ Collects all the data from all the subjects and writes in file """
  # Initiate lists
  X_ = []
  Y_ = []
  for s in range(1,subject_count+1):
    # Read the data
    df = read_subject(s)
    # Split into blocks
    x,y = split_by_blocks(df, block_size)
    X_.append(x)
    Y_.append(y)
  
  return X_, Y_
    
#apply our function
act_list, label_list = collect_save_data(10, 100)

#split data into training and test set
X_train, X_test, y_train, y_test = train_test_split(act_list, label_list, test_size=0.2, random_state=123)

#create clients
def create_clients(act_list, label_list, num_clients=8, initial='clients'):
  #create a list of client names
  client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]
  return {client_names[i] : label_list[i] for i in range(len(client_names))}

clients = create_clients(X_train, y_train, num_clients=8, initial='client')

comms_round = 100
    
#create optimizer
lr = 0.01 
loss='binary_crossentropy'
metrics = [tf.keras.metrics.Accuracy(),f1_m,precision_m, recall_m]
optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)
from keras.layers import Dropout

class SimpleMLP:
  @staticmethod
  def build(shape, classes):
    model = Sequential()
    model.add(Conv1D(46, 2, 1, padding='same', activation="relu", input_shape=(shape,classes)))
    model.add(MaxPooling1D(2, 2, padding='same'))
    model.add(Conv1D(92, 2, 1, padding='same', activation="relu"))
    model.add(MaxPooling1D(2, 2, padding='same'))
    model.add(Conv1D(184, 5, 1, padding='same', activation="relu"))
    model.add(Dropout(0.5))
    model.add(Dense(6))
    model.add(Flatten())
    model.add(Activation("softmax"))
    return model

#initialize global model
smlp_global = SimpleMLP()
global_model = smlp_global.build(184, 6)

def weight_scalling_factor(clients_trn_data, client_name):
  client_names = list(clients_trn_data.keys())
  #get the bs
  bs = list(clients_trn_data[client_name])[0][0].shape[0]
  #first calculate the total training data points across clinets
  global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs
  # get the total number of data points held by a client
  local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs
  return local_count/global_count

def scale_model_weights(weight, scalar):
  '''function for scaling a models weights'''
  weight_final = []
  steps = len(weight)
  for i in range(steps):
    weight_final.append(scalar * weight[i])
  return weight_final

def sum_scaled_weights(scaled_weight_list):
  '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''
  avg_grad = list()
  #get the average grad accross all client gradients
  for grad_list_tuple in zip(*scaled_weight_list):
    layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)
    avg_grad.append(layer_mean)
  return avg_grad

#commence global training loop
for comm_round in range(comms_round):            
  # get the global model's weights - will serve as the initial weights for all local models
  global_weights = global_model.get_weights()
  
  #initial list to collect local model weights after scalling
  scaled_local_weight_list = list()

  #get client data
  client_names = list(clients.keys())
  random.shuffle(client_names)
  
  #loop through each client and create new local model
  for client in client_names:
    smlp_local = SimpleMLP()
    local_model = smlp_local.build(184, 6)
    local_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)
    
    #set local model weight to the weight of the global model
    local_model.set_weights(global_weights)

    # fit the modelwith client's data
    local_model.fit(clients[client], y_train, validation_split=0.2, epochs=10, verbose=0)
    
    #scale the model weights and add to list
    scaling_factor = weight_scalling_factor(clients, client)
    scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)
    scaled_local_weight_list.append(scaled_weights)
    
    #clear session to free memory after each communication round
    K.clear_session()
      
  #to get the average over all the local model, we simply take the sum of the scaled weights
  average_weights = sum_scaled_weights(scaled_local_weight_list)
  
  #update global model 
  global_model.set_weights(average_weights)

  #test global model and print out metrics after each communications round
  for(X_test, y_test) in test_batched:
    global_acc, global_loss = test_model(X_test, y_test, global_model, comm_round)
    SGD_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(y_train)).batch(320)

smlp_SGD = SimpleMLP()
SGD_model = smlp_SGD.build(184, 6) 

SGD_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)

# fit the SGD training data to model
_ = SGD_model.fit(SGD_dataset, X_test, epochs=5, verbose=0)

#test the SGD global model and print out metrics
for(X_test, Y_test) in test_batched:
  SGD_acc, SGD_loss = test_model(X_test, Y_test, SGD_model, 1)

def recall_m(y_train, y_test):
  true_positives = K.sum(K.round(K.clip(y_train * y_test, 0, 1)))
  possible_positives = K.sum(K.round(K.clip(y_train, 0, 1)))
  recall = true_positives / (possible_positives + K.epsilon())
  return recall

def precision_m(y_train, y_test):
  true_positives = K.sum(K.round(K.clip(y_train * y_test, 0, 1)))
  predicted_positives = K.sum(K.round(K.clip(y_test, 0, 1)))
  precision = true_positives / (predicted_positives + K.epsilon())
  return precision

def f1_m(y_train, y_test):
  precision = precision_m(y_train, y_test)
  recall = recall_m(y_train, y_test)
  return 2*((precision*recall)/(precision+recall+K.epsilon()))

# evaluate the model
loss, accuracy, f1_score, precision, recall = SGD_modell.evaluate(X_test, y_test, verbose=0)
